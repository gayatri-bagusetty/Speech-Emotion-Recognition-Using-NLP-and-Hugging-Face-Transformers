# -*- coding: utf-8 -*-
"""NLP Case Study.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYM0a0SGZI0UFtIWBYnn_rdI96J24Hsp

# **SPEECH EMOTION RECOGINITION**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**With pre-trained model(Hugging Face)**"""

!pip install pipwin

!pip install SpeechRecognition

!pip install --user -U nltk

import os
import string
import nltk
import torch
import speech_recognition as sr
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from transformers import pipeline
from datetime import datetime  # Timestamp handling

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt_tab')

# Load Hugging Face emotion detection model
emotion_classifier = pipeline("text-classification", model="bhadresh-savani/distilbert-base-uncased-emotion")

def preprocess_text(text):
    """Clean and preprocess text."""
    text = text.lower()  # Convert to lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    words = word_tokenize(text)  # Tokenize words

    # Handle missing stopwords safely
    try:
        stop_words = set(stopwords.words('english')) - {"not", "no", "nor"}  # Keep negations
    except LookupError:
        nltk.download('stopwords')
        stop_words = set(stopwords.words('english')) - {"not", "no", "nor"}

    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return " ".join(words)

def detect_emotion(text):
    """Detect emotion from text using Hugging Face model."""
    if not text:
        return "No text provided."

    # Preprocess text
    cleaned_text = preprocess_text(text)

    # Get emotion prediction
    emotion = emotion_classifier(cleaned_text)
    return emotion[0]['label']

def speech_to_text(audio_file):
    """Convert speech to text from an audio file."""
    recognizer = sr.Recognizer()
    try:
        with sr.AudioFile(audio_file) as source:
            print(f"üé§ Processing: {audio_file}")
            audio = recognizer.record(source)

        text = recognizer.recognize_google(audio)
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # Get current timestamp
        return text, timestamp
    except sr.UnknownValueError:
        return "Could not understand the audio.", None
    except sr.RequestError as e:
        return f"Speech Recognition API Error: {e}", None

def process_audio_dataset(dataset_folder):
    """Process multiple audio files in a dataset folder."""
    if not os.path.exists(dataset_folder):
        print("Dataset folder not found.")
        return

    for file in os.listdir(dataset_folder):
        if file.endswith(".wav") or file.endswith(".mp3"):
            audio_path = os.path.join(dataset_folder, file)
            spoken_text, speech_time = speech_to_text(audio_path)
            if spoken_text and speech_time:
                detected_emotion = detect_emotion(spoken_text)
                print(f"üìù File: {file} | ‚è±Ô∏è Timestamp: {speech_time} | üó£Ô∏è Text: {spoken_text} | üí° Emotion: {detected_emotion}")

# Set the path to your dataset folder
dataset_folder = "/content/drive/MyDrive/Case Study/audio"  # Change this to your actual folder path
process_audio_dataset(dataset_folder)

"""**Without Pre-Trained models**

SPEECH TO TEXT CONVERSION CODE

Single audio
"""

!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
!unzip vosk-model-small-en-us-0.15.zip

from vosk import Model, KaldiRecognizer
import wave
import json

# Load VOSK model, make sure to rename your 'model' variable
vosk_model = Model("vosk-model-small-en-us-0.15")  # The model is now in the current directory

!ffmpeg -i "/content/drive/MyDrive/Case Study/audio/OSR_us_000_0010_8k.wav" -ac 1 -ar 16000 "/content/drive/MyDrive/Case Study/audio/OSR_us_000_0010_8k_converted.wav"

def speech_to_text_vosk(audio_path):
    """Convert speech to text using VOSK."""
    wf = wave.open(audio_path, "rb")
    rec = KaldiRecognizer(vosk_model, wf.getframerate())

    text = ""
    while True:
        data = wf.readframes(4000)
        if len(data) == 0:
            break
        if rec.AcceptWaveform(data):
            text += json.loads(rec.Result())["text"] + " "

    return text if text.strip() else "Speech not recognized"

# Convert and recognize speech
# Use the path to the *converted* audio file
converted_audio_path = "/content/drive/MyDrive/Case Study/audio/OSR_us_000_0010_8k_converted.wav"
converted_audio = speech_to_text_vosk(converted_audio_path)
print("Extracted Text:", converted_audio) # Print the extracted text from converted audio

"""Dataset"""

!pip install pydub

import os
import wave
import json
import shutil
from vosk import Model, KaldiRecognizer
from pydub import AudioSegment

# Define paths
DATASET_FOLDER = "/content/drive/MyDrive/Case Study/audio"
CONVERTED_FOLDER = "/content/drive/MyDrive/Case Study/audio_converted"

# Ensure converted folder exists
os.makedirs(CONVERTED_FOLDER, exist_ok=True)

# Load VOSK model (make sure the model is in the same directory)
vosk_model = Model("vosk-model-small-en-us-0.15")

def convert_to_16k(audio_path, output_path):
    """Convert audio file to mono 16kHz WAV format."""
    audio = AudioSegment.from_file(audio_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(output_path, format="wav")

def speech_to_text_vosk(audio_path):
    """Convert speech to text using VOSK."""
    wf = wave.open(audio_path, "rb")
    rec = KaldiRecognizer(vosk_model, wf.getframerate())

    text = ""
    while True:
        data = wf.readframes(4000)
        if len(data) == 0:
            break
        if rec.AcceptWaveform(data):
            text += json.loads(rec.Result())["text"] + " "

    return text.strip() if text.strip() else "Speech not recognized"

# Process all audio files in the dataset folder
results = {}
for filename in os.listdir(DATASET_FOLDER):
    if filename.endswith(".wav"):
        input_path = os.path.join(DATASET_FOLDER, filename)
        output_path = os.path.join(CONVERTED_FOLDER, filename.replace(".wav", "_converted.wav"))

        # Convert to 16kHz if needed
        convert_to_16k(input_path, output_path)

        # Perform speech recognition
        extracted_text = speech_to_text_vosk(output_path)
        results[filename] = extracted_text

        print(f"üéµ File: {filename} ‚Üí Extracted Text: {extracted_text}")

# Save results to a text file
output_file = os.path.join(CONVERTED_FOLDER, "speech_recognition_results.txt")
with open(output_file, "w") as f:
    for file, text in results.items():
        f.write(f"File: {file}\nExtracted Text: {text}\n{'-'*50}\n")

print("\n‚úÖ Speech recognition completed. Results saved to:", output_file)

"""TEXT PROCESSING CODE"""

import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK data
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """Clean and preprocess text."""
    text = text.lower()  # Convert to lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    words = word_tokenize(text)  # Tokenization

    # Remove stopwords
    stop_words = set(stopwords.words('english')) - {"not", "no", "nor"}  # Keep negations
    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization

    return " ".join(filtered_words)

def process_text_file(input_path, output_path):
    """Process the text file and save cleaned text."""
    processed_data = []

    with open(input_path, "r", encoding="utf-8") as file:
        lines = file.readlines()

    for line in lines:
        if line.startswith("Extracted Text:"):  # Identify transcribed text
            text = line.replace("Extracted Text:", "").strip()
            processed_text = preprocess_text(text)
            processed_data.append(processed_text)

    # Save processed text to a new file
    with open(output_path, "w", encoding="utf-8") as output_file:
        for i, processed_text in enumerate(processed_data, 1):
            output_file.write(f"Processed Text {i}: {processed_text}\n")

    print(f"‚úÖ Processed text saved to: {output_path}")

# Example usage
input_file = "/content/drive/MyDrive/Case Study/audio_converted/speech_recognition_results.txt"  # Change path if needed
output_file = "/content/drive/MyDrive/Case Study/processed_speech_text.txt"
process_text_file(input_file, output_file)

# Feature Extraction Using TF-IDF
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Load processed text data
input_file = "/content/drive/MyDrive/Case Study/processed_speech_text.txt"

# Read processed text file
texts = []
with open(input_file, "r", encoding="utf-8") as file:
    for line in file:
        texts.append(line.strip().split(": ", 1)[1])  # Extract actual text

# Sample labels (You need labeled data; using placeholders here)
# Make sure the number of labels matches the number of text samples
labels = ["neutral", "happy", "angry", "sad", "neutral", "happy", "sad", "angry", "neutral", "happy", "neutral"]  # Added one more label

# Convert text to TF-IDF features
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)  # Convert text into numerical representation
y = labels  # Emotion labels

# Split dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"‚úÖ TF-IDF Feature Extraction Done. Training Samples: {X_train.shape[0]}, Testing Samples: {X_test.shape[0]}")

# Train an Emotion Classification Model (Naive Bayes)
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Train Naive Bayes Classifier
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Predict on test set
y_pred = nb_model.predict(X_test)

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
print(f"‚úÖ Model Accuracy: {accuracy:.2f}")

# Show classification report
print(classification_report(y_test, y_pred))

# Predict Emotions for New Audio Text
def predict_emotion(new_text):
    """Predict emotion for a given text."""
    processed_text = vectorizer.transform([new_text])  # Convert text to TF-IDF
    predicted_emotion = nb_model.predict(processed_text)[0]
    return predicted_emotion

# Example Prediction
new_audio_text = "i feel very happy today after meeting my best friend"
predicted_emotion = predict_emotion(new_audio_text)
print(f"üé≠ Predicted Emotion: {predicted_emotion}")